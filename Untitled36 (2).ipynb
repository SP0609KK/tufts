{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27af9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "active_report_path_tufts = r'<active report path>\n",
    "\n",
    "df_tufts= pd.read_excel(active_report_path_tufts, sheet_name='Prod Active Report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "tufts_path = r\"\\\\path.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31100ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mns_dict(path):\n",
    "    output_dict = {}\n",
    "    xls = pd.ExcelFile(path)\n",
    "    for sheet in xls.sheet_names:\n",
    "        mns_values = []\n",
    "        text = str(sheet)\n",
    "        # Extract MNS plans from sheet name\n",
    "        mns_matches = re.findall(r'\\b(?:MNS)?\\d+[xX_]\\d+\\b|\\bMNS\\d+\\b|\\b\\d+[xX_]\\d+.*MNS\\b', text)\n",
    "        if mns_matches:\n",
    "            for mns in mns_matches:\n",
    "                # If 'MNS' is already present, remove any additional occurrences\n",
    "                normalized_mns = re.sub(r'\\bMNS\\b', '', mns, count=1) if mns.startswith('MNS') else mns\n",
    "                # Replace 'x' or 'X' with appropriate number of zeros\n",
    "                parts = re.split(r'[xX_]', normalized_mns)\n",
    "                if len(parts) == 2 and parts[1].isdigit():\n",
    "                    if len(parts[1]) == 2:\n",
    "                        normalized_mns = parts[0] + '000' + parts[1]\n",
    "                    elif len(parts[1]) == 1:\n",
    "                        normalized_mns = parts[0] + '0000' + parts[1]\n",
    "                mns_values.append(normalized_mns)\n",
    "            output_dict[sheet] = mns_values\n",
    "        else:\n",
    "            output_dict[sheet] = 'No match found'\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42962822",
   "metadata": {},
   "outputs": [],
   "source": [
    "mns_dict_tufts=mns_dict(tufts_path)\n",
    "mns_dict_tufts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plans_to_audit(book, date):\n",
    "    processing function here\n",
    "    return plan_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9962bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mns_ids = plans_to_audit(TUFTS, '01/01/2024')\n",
    "mns_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sheets(mns_ids, mns_dict):\n",
    "    mns_ids = mns_ids\n",
    "    sheets=[]\n",
    "    \n",
    "    for id in mns_ids:\n",
    "        for key, values in mns_dict.items():\n",
    "            if id in values:\n",
    "                sheets.append(key)\n",
    "    return sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets = fetch_sheets(mns_ids, mns_dict_tufts)\n",
    "sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae579d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (1137919539.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    tufts_path = r\"\\\\path.xlsx\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "def fetch_sot(sheets, mns_dict):\n",
    "    tufts_path = r\"\\\\path.xlsx\n",
    "    \n",
    "    xls_tufts = pd.ExcelFile(tufts_path)\n",
    "    \n",
    "    df_list=[]\n",
    "    \n",
    "    for sheet in sheets:\n",
    "\n",
    "        if any('UNITED HEALTHCARE' in str(cell) for cell in xls_tufts.parse(sheet).values.flatten()):\n",
    "            df=pd.read_excel(tufts_path, sheet_name=sheet, dtype = str)\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                for col in range(len(row)):\n",
    "                # If the cell contains \"PRIMARY/SOLD PLAN\"\n",
    "                    if row[col] == \"PRIMARY/SOLD PLAN\":\n",
    "                    # Fill all cells to the left with the value \"PRIMARY/SOLD PLAN\"\n",
    "                        for i in range(col - 1, -1, -1):\n",
    "                            df.iloc[index, i] = row[col]\n",
    "\n",
    "            unique_options = set()\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                for cell in row:\n",
    "                    if isinstance(cell, str):  # Check if the cell is a string\n",
    "                        options = re.findall(r'OPTION \\d+', cell)  # Extract options using regex\n",
    "                        unique_options.update(options)\n",
    "\n",
    "                # Iterate through the DataFrame rows\n",
    "            for index, row in df.iterrows():\n",
    "                for col in range(len(row)):\n",
    "                        # If the cell contains any of the unique options\n",
    "                    for option in unique_options:\n",
    "                        if isinstance(row[col], str) and option in row[col]:\n",
    "                                # Fill all cells to the left with the corresponding value\n",
    "                            for i in range(col - 1, -1, -1):\n",
    "                                df.iloc[index, i] = row[col]\n",
    "                            break  # Break out of the loop once the option is found\n",
    "\n",
    "                # Find the index where \"GRAND TOTAL\" occurs in any column\n",
    "            grand_total_index = df[df.eq('GRAND TOTAL').any(axis=1)].index[0]\n",
    "\n",
    "                # Drop all rows after the index of \"GRAND TOTAL\"\n",
    "            df = df.iloc[:grand_total_index]\n",
    "\n",
    "                # Keeping only columns Unnamed: 4 to Unnamed: 7\n",
    "            df = df.iloc[:, 4:8]\n",
    "\n",
    "            def shift_non_nan(row):\n",
    "                shifted_row = [np.nan] * len(row)\n",
    "                last_non_nan_index = None\n",
    "                for i, value in enumerate(row):\n",
    "                    if pd.notnull(value):\n",
    "                        if last_non_nan_index is None:\n",
    "                            shifted_row[0] = value\n",
    "                            last_non_nan_index = 0\n",
    "                        else:\n",
    "                            last_non_nan_index += 1\n",
    "                            shifted_row[last_non_nan_index] = value\n",
    "                return pd.Series(shifted_row)\n",
    "\n",
    "                # Apply the function to each row of the dataframe\n",
    "            df = df.apply(shift_non_nan, axis=1)\n",
    "\n",
    "            benefit_plan_rows = df[df.apply(lambda row: 'Benefit Plan' in row.values, axis=1)]\n",
    "\n",
    "                # Merge cells to the right of 'Benefit Plan' rows\n",
    "            for index, row in benefit_plan_rows.iterrows():\n",
    "                    # Find the index of the column where 'Benefit Plan' is located\n",
    "                benefit_plan_index = np.where(row == 'Benefit Plan')[0][0]\n",
    "                    # Find non-NaN values to merge\n",
    "                non_nan_values = row.iloc[benefit_plan_index + 1:].dropna().tolist()\n",
    "                    # Merge non-NaN values into a single cell\n",
    "                merged_value = ' '.join(str(cell) for cell in non_nan_values)\n",
    "                    # Assign the merged value to the cell next to 'Benefit Plan'\n",
    "                df.at[index, benefit_plan_index + 1] = merged_value\n",
    "\n",
    "            df = df.drop(columns=[2, 3])\n",
    "\n",
    "            df = df.dropna()\n",
    "\n",
    "            def split_rows(df):\n",
    "                new_rows = []\n",
    "                for index, row in df.iterrows():\n",
    "                    split_row = False  # Flag to check if splitting occurred for this row\n",
    "                    for column_name, cell_value in row.items():\n",
    "                        if ',' in str(cell_value):\n",
    "                            split_row = True  # Set flag to True if splitting occurred for any column\n",
    "                            details = str(cell_value).split(', ')\n",
    "                            for detail in details:\n",
    "                                new_row = row.copy()  # Copy the original row\n",
    "                                new_row[column_name] = detail.strip()  # Update the cell value\n",
    "                                new_rows.append(new_row)  # Append the new row to the list\n",
    "                            break  # Break out of the loop once splitting occurs for any column\n",
    "                    if not split_row:\n",
    "                        new_rows.append(row)  # Append the original row if no splitting occurred\n",
    "                df = pd.DataFrame(new_rows)  # Creating a new DataFrame with split rows\n",
    "                return df\n",
    "\n",
    "                # Call the function with the DataFrame\n",
    "            df = split_rows(df)\n",
    "\n",
    "            df.index = range(len(df))\n",
    "\n",
    "                # Words to check for deletion\n",
    "            words_to_delete = ['OP', 'IP', 'MD', 'X-Ray', 'Lab', 'Riders', 'Drug Benefit', 'Rating Group']\n",
    "\n",
    "                # Function to filter out rows containing specified words\n",
    "            def filter_rows(df, words_to_delete):\n",
    "                    indices_to_drop = []  # List to store indices of rows to drop\n",
    "                    # Iterate through each row\n",
    "                for index, row in df.iterrows():\n",
    "                        # Flag to check if any word to delete is found\n",
    "                    delete_row = False\n",
    "                        # Iterate through each cell value in the row\n",
    "                    for cell_value in row.values:\n",
    "                            # Check if any word to delete is present in the cell value\n",
    "                        for word in words_to_delete:\n",
    "                            if re.search(r'\\b' + word + r'\\b', str(cell_value)):\n",
    "                                    # If any word is found, set delete_row flag to True\n",
    "                                delete_row = True\n",
    "                                break  # No need to continue checking for words in this row\n",
    "                        if delete_row:\n",
    "                            break  # No need to check further if any word is found in this row\n",
    "                    if delete_row:\n",
    "                        indices_to_drop.append(index)  # Store the index of the row to drop\n",
    "                    # Drop the rows outside of the loop to avoid modifying DataFrame while iterating\n",
    "                df = df.drop(indices_to_drop)\n",
    "                return df\n",
    "\n",
    "                # Call the function with the DataFrame\n",
    "            df = filter_rows(df, words_to_delete)\n",
    "\n",
    "            df = df.applymap(lambda x: str(x).replace('$', ' $') if isinstance(x, str) else x)\n",
    "\n",
    "                # Extracting the words \"PCP\", \"SPC\", \"UC\", \"ER\", \"Ded\", and \"Coins\" into a separate column\n",
    "            df.insert(loc=1, column='Type', value=df[1].str.extract(r'(PCP|SPC|UC|ER|Ded|Coins|OOP)', expand=False))\n",
    "\n",
    "                # Fill NaN values in the \"Type\" column with a default value\n",
    "            df['Type'].fillna('', inplace=True)\n",
    "\n",
    "                # Remove the words \"PCP\", \"SPC\", \"UC\", \"ER\", \"Ded\", and \"Coins\" from the actual column\n",
    "            df[1] = df[1].str.replace(r'(PCP|SPC|UC|ER|Ded|Coins|OOP)', '', regex=True)\n",
    "\n",
    "                # Merge the first and second columns\n",
    "            df.insert(loc=0, column='Merged', value=df[0].astype(str) + \" \" + df['Type'].astype(str))\n",
    "\n",
    "                # Drop the 'Type' and the original first column\n",
    "            df.drop(columns=[0, 'Type'], inplace=True)\n",
    "\n",
    "                # Set column names to empty strings\n",
    "            df.columns = [''] * len(df.columns)\n",
    "\n",
    "                # Reset index\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                # Row mapping dictionary\n",
    "            word_mapping = {\n",
    "                    'In Network Copays PCP': 'INN PCP COPAY',\n",
    "                    'In Network Copays SPC': 'INN SPEC COPAY',\n",
    "                    'In Network Copays UC': 'INN URGENT CARE COPAY',\n",
    "                    'In Network Copays ER': 'INN ER COPAY',\n",
    "                    'In Net Ded/Coins/OOP Ded': 'INN DEDUCTIBLE',\n",
    "                    'In Net Ded/Coins/OOP Coins': 'INN COINSURANCE',\n",
    "                    'In Net Ded/Coins/OOP OOP': 'INN OOP Max.',\n",
    "                    'Out of Network Ded': 'OON DEDUCTIBLE',\n",
    "                    'Out of Network Coins': 'OON COINSURANCE',\n",
    "                    'Out of Network OOP': 'OON OOP Max.'\n",
    "            }\n",
    "\n",
    "                # Replace words in the DataFrame using the mapping dictionary\n",
    "            df.replace(word_mapping, inplace=True)\n",
    "\n",
    "                # Create a new DataFrame to store the result\n",
    "            new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "                # Iterate over each row in the original DataFrame\n",
    "            for index, row in df.iterrows():\n",
    "                    # Check if the row in the second column contains '/'\n",
    "                if '/' in row[1]:  # Assuming the second column is the one you want to check\n",
    "                        # Split the row into two rows if '/' has '$' before or after it\n",
    "                    parts = row[1].split('/')\n",
    "                    if ('$' in parts[0]) or ('$' in parts[1]):\n",
    "                            # Add 'IND' prefix to the first split row and 'FAM' to the second split row if necessary\n",
    "                        new_df.loc[len(new_df)] = ['IND ' + row[0], parts[0]]\n",
    "                        new_df.loc[len(new_df)] = ['FAM ' + row[0], parts[1]]\n",
    "                    else:\n",
    "                        new_df.loc[len(new_df)] = [row[0], row[1]]\n",
    "                else:\n",
    "                        # If no '/', append the row as it is to the new DataFrame\n",
    "                    new_df.loc[len(new_df)] = [row[0], row[1]]\n",
    "\n",
    "                split_dfs = []\n",
    "\n",
    "                # Iterate through the processed_data DataFrame\n",
    "            current_df = None\n",
    "            for index, row in processed_data.iterrows():\n",
    "                    # Check if the row starts with \"OPTION\"\n",
    "                if row[0].startswith('OPTION'):\n",
    "                        # Check if there is an existing current_df and if it meets the length condition\n",
    "                    if current_df is not None and len(current_df) >= 6:\n",
    "                            # Append the current_df to split_dfs\n",
    "                        split_dfs.append(current_df)\n",
    "                        # Create a new DataFrame for the current section\n",
    "                    current_df = pd.DataFrame(columns=processed_data.columns)\n",
    "                    # Append the row to the current_df if it exists\n",
    "                if current_df is not None:\n",
    "                    current_df = pd.concat([current_df, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "                # Check if there is a current_df at the end and if it meets the length condition\n",
    "            if current_df is not None and len(current_df) >= 6:\n",
    "                    # Append the current_df to split_dfs\n",
    "                split_dfs.append(current_df)\n",
    "\n",
    "            mns_id = mns_dict.get(sheet, 'MNS ID not found')  \n",
    "            if mns_id != 'MNS ID not found':\n",
    "                for df_split in split_dfs:\n",
    "                        # Create a DataFrame with MNS ID row\n",
    "                    mns_id_row = pd.DataFrame([['MNS ID', mns_id]], columns=df_split.columns)\n",
    "                        # Concatenate the MNS ID row with the split DataFrame\n",
    "                    df_split = pd.concat([mns_id_row, df_split], ignore_index=True)\n",
    "                        # Append the split DataFrame to df_list\n",
    "                    df_list.append(df_split)\n",
    "            else:\n",
    "                print(f\"MNS ID not found for sheet '{sheet}'\")\n",
    "                \n",
    "        elif any('Freedom' in str(cell) for cell in xls_tufts.parse(sheet).values.flatten()):\n",
    "            df=pd.read_excel(tufts_path, sheet_name=sheet, dtype = str)\n",
    "            null_percentage = df.isnull().mean()*100\n",
    "            cols_to_drop = null_percentage[null_percentage > 94].index\n",
    "            df = df.drop(columns=cols_to_drop)\n",
    "            \n",
    "            phrases_to_keep = ['Annual Medical Deductible', 'Individual', 'Family', 'Annual Out-of-Pocket Limit', 'Primary Care Physician', 'Specialist', 'Urgent Care Center Services', 'Emergency Care']\n",
    "            df = df[df.iloc[:, 0].isin(phrases_to_keep)]\n",
    "            \n",
    "            df = df.dropna(axis=1, how='all')\n",
    "            \n",
    "            new_col_names = {0: 'Field', 1: 'In Network', 2:'Out of Network'}\n",
    "            \n",
    "            if len(df.columns) <= 1:\n",
    "                # Skip further processing\n",
    "                print(f\"Sheet '{sheet}' was skipped because it has only one column.\")\n",
    "                continue\n",
    "                \n",
    "            for idx, col_name in new_col_names.items():\n",
    "                if idx < len(df.columns):\n",
    "                    df = df.rename(columns={df.columns[idx]: col_name})\n",
    "                    \n",
    "            if 'Out of Network' not in df.columns:\n",
    "                result_df = df.copy()\n",
    "            else:\n",
    "                df1 = pd.DataFrame({'Field':['In Network']})\n",
    "                df1 = pd.concat([df1, df[['Field', 'In Network']]])\n",
    "\n",
    "                df2 = pd.DataFrame({'Fields':['Out of Network']})\n",
    "                df2 = pd.concat([df2, df[['Field', 'In Network']]])\n",
    "\n",
    "                result_df = pd.concat([df1, df2], ignore_index=True)\n",
    "                \n",
    "            result_df_filled = result_df.iloc[:, ::-1].fillna(method='ffill', axis=1).iloc[:,::-1]\n",
    "            \n",
    "            result_df_filled = result_df_filled.drop(['Fields', 'Out of Network'], axis=1. errors='ignore')\n",
    "            result_df_filled = result_df_filled.rename(columns={'In Network': 'Value'})\n",
    "            \n",
    "            for i, row in result_df_filled.iterrows():\n",
    "                if row['Field']=='Annual Medical Deductible':\n",
    "                    result_df_filled.at[i+1, 'Field']= 'Individual Deductible'\n",
    "                    result_df_filled.at[i+2, 'Field']= 'Family Deductible'\n",
    "                elif row['Field']=='Annual Out-of-Pocket Limit':\n",
    "                    result_df_filled.at[i+1, 'Field']= 'Individual OOP'\n",
    "                    result_df_filled.at[i+2, 'Field']= 'Family OOP'\n",
    "\n",
    "            result_df_filled = result_df_filled[~result_df_filled['Field'].isin(['Annual Medical Deductible', 'Annual Out-of-Pocket Limit'])]\n",
    "            \n",
    "            result_df_filled.insert(0, 'Network Type', result_df_filled['Fields'].apply(lambda x: 'In Network' if x == 'In Network' else 'Out of Network' if x == 'Out of Network' else ''))\n",
    "            \n",
    "            # Forward fill \"In Network\" and \"Out of Network\" values until the next column contains \"Effective Date\", \"Thru Date\", or \"Network\"\n",
    "            network_col_index = result_df_filledf.columns.get_loc('Network Type')\n",
    "            values_col_index = result_df_filled.columns.get_loc('Field')\n",
    "            for i in range(len(result_df_filled)):\n",
    "                if result_df_filled.iloc[i, network_col_index] in ['In Network', 'Out of Network']:\n",
    "                    for j in range(i, len(result_df_filled)):\n",
    "                        if result_df_filled.iloc[j, values_col_index] in ['Primary Care Physician', 'Specialist', 'Urgent Care Center Services', 'Emergency Care']:\n",
    "                            break\n",
    "                        else:\n",
    "                            result_df_filled.iloc[j, network_col_index] = result_df_filled.iloc[i, network_col_index]\n",
    "                            \n",
    "            # Remove rows where values are repeated within the same row\n",
    "            result_df_filled = result_df_filled[~result_df_filled.apply(lambda row: len(set(row.dropna())) != len(row.dropna()), axis=1)]\n",
    "\n",
    "                        # Merge first and second columns\n",
    "            result_df_filled.iloc[:, 0] = result_df_filled.iloc[:, 0] + ' ' + result_df_filled.iloc[:, 1]\n",
    "\n",
    "            # Drop the second column\n",
    "            result_df_filled.drop(columns=[result_df_filled.columns[1]], inplace=True)\n",
    "            \n",
    "            for i, row in result_df_filled.iterrows():\n",
    "                if '%' in str(row['Value']):\n",
    "                    result_df_filled.at[i, 'Network Type']='Coinsurance'\n",
    "                    \n",
    "                    \n",
    "        else:\n",
    "            print('Sheet not found in SOT')\n",
    "            \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c96d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_sot(sheets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
