{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def process_excel_from_folder(file_path):\n",
    "    def extract_tables_from_excel(file_path, output_dir):\n",
    "        start_string = 'UNITED HEALTHCARE UNDERWRITING FULLY INSURED APPROVAL FORM - For Internal Use Only'\n",
    "        end_string = 'GRAND TOTAL'\n",
    "\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name, header=None, skipfooter=1, dtype=str)\n",
    "            table_started = False\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                if start_string in ' '.join(map(str, row)):\n",
    "                    start_row_index = index\n",
    "                    table_started = True\n",
    "                elif table_started and end_string in ' '.join(map(str, row)):\n",
    "                    end_row_index = index\n",
    "                    selected_table = df.iloc[start_row_index:end_row_index + 1]\n",
    "\n",
    "                    output_file_path = os.path.join(output_dir, f\"{sheet_name}.xlsx\")\n",
    "                    selected_table.to_excel(output_file_path, index=False, header=False)\n",
    "\n",
    "                    wb = load_workbook(output_file_path)\n",
    "                    ws = wb.active\n",
    "                    ws.title = sheet_name\n",
    "                    wb.save(output_file_path)\n",
    "\n",
    "                    table_started = False\n",
    "                    break\n",
    "\n",
    "    def process_excel_sheet(folder_path, group_name):\n",
    "    def process_data(data):\n",
    "        # Identify datetime columns\n",
    "        date_columns = data.select_dtypes(include=[np.datetime64]).columns\n",
    "\n",
    "        # Convert datetime columns to string\n",
    "        data[date_columns] = data[date_columns].apply(lambda x: x.astype(str) if x.name in date_columns else x)\n",
    "\n",
    "        # Extracting sheet name without \"MNS\"\n",
    "        filtered_sheet_name = re.sub(r'\\bMNS.*', '', sheet_name).strip()\n",
    "\n",
    "        # Create a DataFrame for the filtered sheet name with the same number of columns as data\n",
    "        filtered_sheet_name_df = pd.DataFrame([filtered_sheet_name] * len(data.columns)).T\n",
    "        filtered_sheet_name_df.columns = data.columns\n",
    "\n",
    "        # Concatenate filtered sheet name DataFrame with the original data\n",
    "        data = pd.concat([filtered_sheet_name_df, data], ignore_index=True)\n",
    "\n",
    "        # Iterate through the DataFrame rows\n",
    "        for index, row in data.iterrows():\n",
    "            for col in range(len(row)):\n",
    "                # If the cell contains \"PRIMARY/SOLD PLAN\"\n",
    "                if row[col] == \"PRIMARY/SOLD PLAN\":\n",
    "                    # Fill all cells to the left with the value \"PRIMARY/SOLD PLAN\"\n",
    "                    for i in range(col - 1, -1, -1):\n",
    "                        data.iloc[index, i] = row[col]\n",
    "\n",
    "        # Extract unique options from the DataFrame\n",
    "        unique_options = set()\n",
    "        for index, row in data.iterrows():\n",
    "            for cell in row:\n",
    "                if isinstance(cell, str):  # Check if the cell is a string\n",
    "                    options = re.findall(r'OPTION \\d+', cell)  # Extract options using regex\n",
    "                    unique_options.update(options)\n",
    "\n",
    "        # Iterate through the DataFrame rows\n",
    "        for index, row in data.iterrows():\n",
    "            for col in range(len(row)):\n",
    "                # If the cell contains any of the unique options\n",
    "                for option in unique_options:\n",
    "                    if isinstance(row[col], str) and option in row[col]:\n",
    "                        # Fill all cells to the left with the corresponding value\n",
    "                        for i in range(col - 1, -1, -1):\n",
    "                            data.iloc[index, i] = row[col]\n",
    "                        break  # Break out of the loop once the option is found\n",
    "\n",
    "        # Keeping only columns Unnamed: 4 to Unnamed: 7\n",
    "        df = data.iloc[:, 4:8]\n",
    "\n",
    "        # Define a function to shift non-NaN values to the left\n",
    "        def shift_non_nan(row):\n",
    "            shifted_row = [np.nan] * len(row)\n",
    "            last_non_nan_index = None\n",
    "            for i, value in enumerate(row):\n",
    "                if pd.notnull(value):\n",
    "                    if last_non_nan_index is None:\n",
    "                        shifted_row[0] = value\n",
    "                        last_non_nan_index = 0\n",
    "                    else:\n",
    "                        last_non_nan_index += 1\n",
    "                        shifted_row[last_non_nan_index] = value\n",
    "            return pd.Series(shifted_row)\n",
    "\n",
    "        # Apply the function to each row of the dataframe\n",
    "        df = df.apply(shift_non_nan, axis=1)\n",
    "\n",
    "        # Find the rows where 'Benefit Plan' is mentioned\n",
    "        benefit_plan_rows = df[df.apply(lambda row: 'Benefit Plan' in row.values, axis=1)]\n",
    "\n",
    "        # Merge cells to the right of 'Benefit Plan' rows\n",
    "        for index, row in benefit_plan_rows.iterrows():\n",
    "            # Find the index of the column where 'Benefit Plan' is located\n",
    "            benefit_plan_index = np.where(row == 'Benefit Plan')[0][0]\n",
    "            # Find non-NaN values to merge\n",
    "            non_nan_values = row.iloc[benefit_plan_index + 1:].dropna().tolist()\n",
    "            # Merge non-NaN values into a single cell\n",
    "            merged_value = ' '.join(str(cell) for cell in non_nan_values)\n",
    "            # Assign the merged value to the cell next to 'Benefit Plan'\n",
    "            df.at[index, benefit_plan_index + 1] = merged_value\n",
    "\n",
    "        # Assuming your DataFrame is named df\n",
    "        df = df.drop(columns=[2, 3])\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Function to split cells containing commas into separate rows for applicable columns\n",
    "        def split_rows(df):\n",
    "            new_rows = []\n",
    "            for index, row in df.iterrows():\n",
    "                split_row = False  # Flag to check if splitting occurred for this row\n",
    "                for column_name, cell_value in row.items():\n",
    "                    if ',' in str(cell_value):\n",
    "                        split_row = True  # Set flag to True if splitting occurred for any column\n",
    "                        details = str(cell_value).split(', ')\n",
    "                        for detail in details:\n",
    "                            new_row = row.copy()  # Copy the original row\n",
    "                            new_row[column_name] = detail.strip()  # Update the cell value\n",
    "                            new_rows.append(new_row)  # Append the new row to the list\n",
    "                        break  # Break out of the loop once splitting occurs for any column\n",
    "                if not split_row:\n",
    "                    new_rows.append(row)  # Append the original row if no splitting occurred\n",
    "            new_df = pd.DataFrame(new_rows)  # Creating a new DataFrame with split rows\n",
    "            return new_df\n",
    "\n",
    "        # Call the function with the DataFrame\n",
    "        df = split_rows(df)\n",
    "\n",
    "        # Assign unique indices\n",
    "        df.index = range(len(df))\n",
    "\n",
    "        # Words to check for deletion\n",
    "        words_to_delete = ['OP', 'IP', 'MD', 'X-Ray', 'Lab', 'Riders', 'Drug Benefit', 'Rating Group']\n",
    "\n",
    "        # Function to filter out rows containing specified words\n",
    "        def filter_rows(df, words_to_delete):\n",
    "            indices_to_drop = []  # List to store indices of rows to drop\n",
    "            # Iterate through each row\n",
    "            for index, row in df.iterrows():\n",
    "                # Flag to check if any word to delete is found\n",
    "                delete_row = False\n",
    "                # Iterate through each cell value in the row\n",
    "                for cell_value in row.values:\n",
    "                    # Check if any word to delete is present in the cell value\n",
    "                    for word in words_to_delete:\n",
    "                        if re.search(r'\\b' + word + r'\\b', str(cell_value)):\n",
    "                            # If any word is found, set delete_row flag to True\n",
    "                            delete_row = True\n",
    "                            break  # No need to continue checking for words in this row\n",
    "                    if delete_row:\n",
    "                        break  # No need to check further if any word is found in this row\n",
    "                if delete_row:\n",
    "                    indices_to_drop.append(index)  # Store the index of the row to drop\n",
    "            # Drop the rows outside of the loop to avoid modifying DataFrame while iterating\n",
    "            df_filtered = df.drop(indices_to_drop)\n",
    "            return df_filtered\n",
    "\n",
    "        # Call the function with the DataFrame\n",
    "        df = filter_rows(df, words_to_delete)\n",
    "\n",
    "        # Add a space before each $ sign in the DataFrame\n",
    "        df = df.applymap(lambda x: str(x).replace('$', ' $') if isinstance(x, str) else x)\n",
    "\n",
    "        # Extracting the words \"PCP\", \"SPC\", \"UC\", \"ER\", \"Ded\", and \"Coins\" into a separate column\n",
    "        df.insert(loc=1, column='Type', value=df[1].str.extract(r'(PCP|SPC|UC|ER|Ded|Coins|OOP)', expand=False))\n",
    "\n",
    "        # Fill NaN values in the \"Type\" column with a default value\n",
    "        df['Type'].fillna('', inplace=True)\n",
    "\n",
    "        # Remove the words \"PCP\", \"SPC\", \"UC\", \"ER\", \"Ded\", and \"Coins\" from the actual column\n",
    "        df[1] = df[1].str.replace(r'(PCP|SPC|UC|ER|Ded|Coins|OOP)', '', regex=True)\n",
    "\n",
    "        # Merge the first and second columns\n",
    "        df.insert(loc=0, column='Merged', value=df[0].astype(str) + \" \" + df['Type'].astype(str))\n",
    "\n",
    "        # Drop the 'Type' and the original first column\n",
    "        df.drop(columns=[0, 'Type'], inplace=True)\n",
    "\n",
    "        # Set column names to empty strings\n",
    "        df.columns = [''] * len(df.columns)\n",
    "\n",
    "        # Reset index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Fill the first NaN value in the first column with 'Group Name'\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].fillna('Group Name', limit=1)\n",
    "\n",
    "        # Fill the second NaN value in the first column with 'MNS ID'\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].fillna('MNS ID', limit=1)\n",
    "\n",
    "        # Set column names to empty strings\n",
    "        df.columns = [''] * len(df.columns)\n",
    "\n",
    "        # Reset index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Replace words in the DataFrame using the mapping dictionary\n",
    "        word_mapping = {\n",
    "            'In Network Copays PCP': 'INN PCP COPAY',\n",
    "            'In Network Copays SPC': 'INN SPEC COPAY',\n",
    "            'In Network Copays UC': 'INN URGENT CARE COPAY',\n",
    "            'In Network Copays ER': 'INN ER COPAY',\n",
    "            'In Net Ded/Coins/OOP Ded': 'INN DEDUCTIBLE',\n",
    "            'In Net Ded/Coins/OOP Coins': 'INN COINSURANCE',\n",
    "            'In Net Ded/Coins/OOP OOP': 'INN OOP Max.',\n",
    "            'Out of Network Ded': 'OON DEDUCTIBLE',\n",
    "            'Out of Network Coins': 'OON COINSURANCE',\n",
    "            'Out of Network OOP': 'OON OOP Max.'\n",
    "        }\n",
    "\n",
    "        # Replace words in the DataFrame using the mapping dictionary\n",
    "        df.replace(word_mapping, inplace=True)\n",
    "\n",
    "        # Create a new DataFrame to store the result\n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "        # Iterate over each row in the original DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            # Check if the row in the second column contains '/'\n",
    "            if '/' in row[1]:  # Assuming the second column is the one you want to check\n",
    "                # Split the row into two rows if '/' has '$' before or after it\n",
    "                parts = row[1].split('/')\n",
    "                if ('$' in parts[0]) or ('$' in parts[1]):\n",
    "                    # Add 'IND' prefix to the first split row and 'FAM' to the second split row if necessary\n",
    "                    new_df.loc[len(new_df)] = ['IND ' + row[0], parts[0]]\n",
    "                    new_df.loc[len(new_df)] = ['FAM ' + row[0], parts[1]]\n",
    "                else:\n",
    "                    new_df.loc[len(new_df)] = [row[0], row[1]]\n",
    "            else:\n",
    "                # If no '/', append the row as it is to the new DataFrame\n",
    "                new_df.loc[len(new_df)] = [row[0], row[1]]\n",
    "\n",
    "        # Return the processed DataFrame\n",
    "        return new_df\n",
    "\n",
    "    result_dfs = []\n",
    "    unique_dfs = set()\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Skip files starting with '~$'\n",
    "        if not file_name.startswith('~$'):\n",
    "            if file_name.endswith('.xlsx'):\n",
    "                # Read the Excel file\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                try:\n",
    "                    # Read all sheets from the Excel file\n",
    "                    sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "                    # Iterate over each sheet\n",
    "                    for sheet_name, data in sheets.items():\n",
    "                        # Check if the sheet_name matches the specified group_name\n",
    "                        if group_name in sheet_name:\n",
    "                            # Process the data for the current sheet\n",
    "                            processed_data = process_data(data)\n",
    "                            # Replace the first row, first column with \"Name\"\n",
    "                            processed_data.iloc[0, 0] = \"Name\"\n",
    "\n",
    "                            # Create a new DataFrame containing rows indexed 0 and 1\n",
    "                            header_df = processed_data.iloc[:2]\n",
    "\n",
    "                            # Split the DataFrame based on rows starting with \"OPTION\"\n",
    "                            split_dfs = []\n",
    "                            current_df = None\n",
    "                            for index, row in processed_data.iterrows():\n",
    "                                if row[0].startswith('OPTION'):\n",
    "                                    if current_df is not None and len(current_df) >= 6:  # Check length condition\n",
    "                                        combined_df = pd.concat([header_df, current_df], ignore_index=True)\n",
    "                                        if combined_df.to_string() not in unique_dfs:\n",
    "                                            split_dfs.append(combined_df)\n",
    "                                            unique_dfs.add(combined_df.to_string())\n",
    "                                        # Concatenate current_df with header_df before appending\n",
    "                                    current_df = pd.DataFrame(columns=processed_data.columns)\n",
    "                                if current_df is not None:\n",
    "                                    current_df = pd.concat([current_df, row.to_frame().T], ignore_index=True)\n",
    "                            if current_df is not None and len(current_df) >= 6:  # Check length condition\n",
    "                                combined_df = pd.concat([header_df, current_df], ignore_index=True)\n",
    "                                if combined_df.to_string() not in unique_dfs:\n",
    "                                    split_dfs.append(combined_df)\n",
    "                                    unique_dfs.add(combined_df.to_string())\n",
    "\n",
    "                            result_dfs.extend(split_dfs)  # Add split DataFrames to the result list\n",
    "\n",
    "                except PermissionError as e:\n",
    "                    print(f\"Permission error occurred while accessing {file_name}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while processing {file_name}: {e}\")\n",
    "\n",
    "    return result_dfs\n",
    "\n",
    "\n",
    "    # Execution of both functions\n",
    "    output_dir = os.path.join(os.path.dirname(file_path), \"Extracted_Tables_Tufts_formattt\")\n",
    "    extract_tables_from_excel(file_path, output_dir)\n",
    "    \n",
    "    result_dfs = []\n",
    "    unique_dfs = set()\n",
    "\n",
    "    for file_name in os.listdir(output_dir):\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            try:\n",
    "                file_path = os.path.join(output_dir, file_name)\n",
    "                sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "                for sheet_name, data in sheets.items():\n",
    "                    if group_name in sheet_name:\n",
    "                        processed_data = process_data(data)\n",
    "                        processed_data.iloc[0, 0] = \"Name\"\n",
    "                        header_df = processed_data.iloc[:2]\n",
    "                        split_dfs = []\n",
    "                        current_df = None\n",
    "                        for index, row in processed_data.iterrows():\n",
    "                            if row[0].startswith('OPTION'):\n",
    "                                if current_df is not None and len(current_df) >= 6:\n",
    "                                    combined_df = pd.concat([header_df, current_df], ignore_index=True)\n",
    "                                    if combined_df.to_string() not in unique_dfs:\n",
    "                                        split_dfs.append(combined_df)\n",
    "                                        unique_dfs.add(combined_df.to_string())\n",
    "                                current_df = pd.DataFrame(columns=processed_data.columns)\n",
    "                            if current_df is not None:\n",
    "                                current_df = pd.concat([current_df, row.to_frame().T], ignore_index=True)\n",
    "                        if current_df is not None and len(current_df) >= 6:\n",
    "                            combined_df = pd.concat([header_df, current_df], ignore_index=True)\n",
    "                            if combined_df.to_string() not in unique_dfs:\n",
    "                                split_dfs.append(combined_df)\n",
    "                                unique_dfs.add(combined_df.to_string())\n",
    "                        result_dfs.extend(split_dfs)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {file_name}: {e}\")\n",
    "\n",
    "    return result_dfs\n",
    "\n",
    "# Example usage:\n",
    "file_path = r\"C:\\Users\\shres\\Downloads\\TUFTS 4 typess.xlsx\"\n",
    "result_dfs = process_excel_from_folder(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
